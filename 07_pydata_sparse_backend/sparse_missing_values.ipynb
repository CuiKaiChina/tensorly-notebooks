{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse PARAFAC with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on [sparse_demo.ipynb](sparse_demo.ipynb#parafac). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we start with a random sparse tensor, constructed so that it has a tensor factorization of rank 5.\n",
    "\n",
    "Because masked PARAFAC can take longer to converge than non-masked PARAFAC, we will use a smaller tensor than in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1000, 1001, 1002)\n",
    "rank = 5\n",
    "\n",
    "import sparse\n",
    "starting_factors = [sparse.random((i, rank)) for i in shape]\n",
    "starting_factors\n",
    "starting_weights = sparse.ones(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th style=\"text-align: left\">Format</th><td style=\"text-align: left\">coo</td></tr><tr><th style=\"text-align: left\">Data Type</th><td style=\"text-align: left\">float64</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(1000, 1001, 1002)</td></tr><tr><th style=\"text-align: left\">nnz</th><td style=\"text-align: left\">5094</td></tr><tr><th style=\"text-align: left\">Density</th><td style=\"text-align: left\">5.0787535817475934e-06</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Size</th><td style=\"text-align: left\">159.2K</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">0.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<COO: shape=(1000, 1001, 1002), dtype=float64, nnz=5094, fill_value=0.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorly.contrib.sparse.kruskal_tensor import kruskal_to_tensor\n",
    "tensor = kruskal_to_tensor((starting_weights, starting_factors))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000163008"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.nbytes / 1e9                # Actual memory usage in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.024016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.prod(tensor.shape) * 8 / 1e9    # Memory usage if array was dense, in GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct a random mask. A mask should be a boolean array of the same shape as the tensor, that is `False` (`0`) where there are missing values and `True` (`1`) where elements are not missing. \n",
    "\n",
    "It is important that the mask array have a fill value of `True`, that is, the zero entries of the original `tensor` should be considered non-missing. This is because internally the parafac algorithm generates dense arrays with as many elements as are False in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th style=\"text-align: left\">Format</th><td style=\"text-align: left\">coo</td></tr><tr><th style=\"text-align: left\">Data Type</th><td style=\"text-align: left\">bool</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(1000, 1001, 1002)</td></tr><tr><th style=\"text-align: left\">nnz</th><td style=\"text-align: left\">1503</td></tr><tr><th style=\"text-align: left\">Density</th><td style=\"text-align: left\">1.4985014985014984e-06</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Size</th><td style=\"text-align: left\">36.7K</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">0.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<COO: shape=(1000, 1001, 1002), dtype=bool, nnz=1503, fill_value=True>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparse\n",
    "missing_p = 0.3 # The fraction of nonzero entries from tensor which should be considered missing. The larger this number is, the harder it will be for PARAFAC to reconstruct the factors (meaning it may take more iterations to converge).\n",
    "\n",
    "mask = sparse.COO(coords=tensor.coords, data=np.random.choice([False, True], size=tensor.nnz, p=[missing_p, 1-missing_p]), shape=tensor.shape, fill_value=True)\n",
    "# This clears the True values from the mask.data\n",
    "mask = sparse.elemwise(lambda x: x, mask)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we factor the tensor. In order to demonstrate that there are no tricks up our sleeve, we multiply the tensor by the mask to clear the \"missing\" values. The mask is passed in as a keyword argument to `parafac()`. \n",
    "\n",
    "Note that at this time, you have to use the `parafac` function from the sparse backend when using a sparse mask to avoid memory blowups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%load_ext memory_profiler\n",
    "from tensorly.contrib.sparse.decomposition import parafac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error=0.8175833368555955\n",
      "iteration 1, reconstruction error: 0.4582569541516581, decrease = 0.35932638270393735, unnormalized = 6.059446087313438\n",
      "iteration 2, reconstruction error: 0.37173227790593805, decrease = 0.08652467624572008, unnormalized = 5.0898443578716845\n",
      "iteration 3, reconstruction error: 0.3433681340822257, decrease = 0.02836414382371233, unnormalized = 4.777701968504902\n",
      "iteration 4, reconstruction error: 0.33827195176982416, decrease = 0.005096182312401554, unnormalized = 4.738296921651478\n",
      "iteration 5, reconstruction error: 0.33671694244574846, decrease = 0.0015550093240757068, unnormalized = 4.730505969337963\n",
      "iteration 6, reconstruction error: 0.3360651876425429, decrease = 0.000651754803205562, unnormalized = 4.728142499505644\n",
      "iteration 7, reconstruction error: 0.33575261109873816, decrease = 0.0003125765438047323, unnormalized = 4.7273045932585855\n",
      "iteration 8, reconstruction error: 0.33558942206153664, decrease = 0.00016318903720152766, unnormalized = 4.726986270648904\n",
      "iteration 9, reconstruction error: 0.3354987585186209, decrease = 9.066354291575873e-05, unnormalized = 4.726859393444556\n",
      "iteration 10, reconstruction error: 0.3354460427075391, decrease = 5.2715811081793795e-05, unnormalized = 4.7268069902739445\n",
      "iteration 11, reconstruction error: 0.33541435208868, decrease = 3.1690618859103736e-05, unnormalized = 4.7267847518391966\n",
      "iteration 12, reconstruction error: 0.335394824933425, decrease = 1.952715525499471e-05, unnormalized = 4.726775117128267\n",
      "iteration 13, reconstruction error: 0.3353825669443114, decrease = 1.2257989113584333e-05, unnormalized = 4.72677087694965\n",
      "iteration 14, reconstruction error: 0.33537476141854716, decrease = 7.80552576423732e-06, unnormalized = 4.726768988771952\n",
      "iteration 15, reconstruction error: 0.3353697351613719, decrease = 5.0262571752446306e-06, unnormalized = 4.726768140549745\n",
      "iteration 16, reconstruction error: 0.33536646959189953, decrease = 3.265569472388208e-06, unnormalized = 4.726767757022429\n",
      "iteration 17, reconstruction error: 0.3353643326148627, decrease = 2.1369770368262486e-06, unnormalized = 4.726767582776012\n",
      "iteration 18, reconstruction error: 0.3353629259429611, decrease = 1.4066719016270213e-06, unnormalized = 4.7267675033314065\n",
      "iteration 19, reconstruction error: 0.33536199550820983, decrease = 9.304347512451017e-07, unnormalized = 4.726767467015747\n",
      "iteration 20, reconstruction error: 0.33536137760792734, decrease = 6.179002824957536e-07, unnormalized = 4.726767450383329\n",
      "iteration 21, reconstruction error: 0.3353609658910165, decrease = 4.117169108419638e-07, unnormalized = 4.726767442754979\n",
      "iteration 22, reconstruction error: 0.33536069079194136, decrease = 2.750990751310134e-07, unnormalized = 4.726767439252632\n",
      "iteration 23, reconstruction error: 0.3353605065474931, decrease = 1.8424444825360098e-07, unnormalized = 4.7267674376433835\n",
      "iteration 24, reconstruction error: 0.33536038290912745, decrease = 1.236383656566531e-07, unnormalized = 4.726767436903546\n",
      "iteration 25, reconstruction error: 0.33536029980323545, decrease = 8.310589200144491e-08, unnormalized = 4.726767436563269\n",
      "iteration 26, reconstruction error: 0.33536024386373486, decrease = 5.5939500587243174e-08, unnormalized = 4.726767436406712\n",
      "iteration 27, reconstruction error: 0.33536020616555695, decrease = 3.76981779126595e-08, unnormalized = 4.726767436334668\n",
      "iteration 28, reconstruction error: 0.3353601807348155, decrease = 2.5430741457110884e-08, unnormalized = 4.726767436301507\n",
      "iteration 29, reconstruction error: 0.33536016356487025, decrease = 1.7169945243367124e-08, unnormalized = 4.726767436286242\n",
      "iteration 30, reconstruction error: 0.33536015196388896, decrease = 1.1600981286719048e-08, unnormalized = 4.726767436279215\n",
      "iteration 31, reconstruction error: 0.3353601441207521, decrease = 7.843136862195621e-09, unnormalized = 4.7267674362759795\n",
      "PARAFAC converged after 31 iterations\n",
      "Took 0 mins 41 secs\n",
      "peak memory: 153.63 MiB, increment: 5.23 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "start_time = time.time()\n",
    "sparse_kruskal = parafac(tensor*mask, rank=rank, init='random', verbose=True, mask=mask)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print('Took %d mins %d secs' % (divmod(total_time, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the values that was masked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  59, 292])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.coords.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[tuple(mask.coords.T[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09704512781634266"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_val = tensor[tuple(mask.coords.T[0])]\n",
    "orig_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [sparse_demo.ipynb](sparse_demo.ipynb) for how to calculate individual values from the factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09704496725694943"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, factors = sparse_kruskal\n",
    "computed_val = np.sum(np.prod(sparse.stack([factors[i][idx] for i, idx in enumerate(tuple(mask.coords.T[0]))], 0), 0))\n",
    "computed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6055939322523471e-07"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(orig_val - computed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this is a constructed example, where we know the unmasked tensor has an exact factorization. But this demonstrates that given a tensor with missing values, which we have reason to believe is represented by a rank $r$ tensor decomposition, we should expect this decomposition to do a decent job at reconstructing those missing values (this may not be the case if the missing values are not randomly distributed across the tensor as we have here). \n",
    "\n",
    "Let's compare this to a value that was not masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tensor.coords.T:\n",
    "    non_missing_coord = tuple(i)\n",
    "    if mask[non_missing_coord]:\n",
    "        break\n",
    "        \n",
    "mask[non_missing_coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0704013608737617"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_val = tensor[non_missing_coord]\n",
    "orig_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07040150152751701"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_val = np.sum(np.prod(sparse.stack([factors[i][idx] for i, idx in enumerate(non_missing_coord)], 0), 0))\n",
    "computed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4065375530947222e-07"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(orig_val - computed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we should not in general try to recompose a sparse factorization unless we can represent it densely, but since this was constructed explicitly from sparse factors, we are able to do it (being careful to use the `kruskal_to_tensor` from the sparse backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th style=\"text-align: left\">Format</th><td style=\"text-align: left\">coo</td></tr><tr><th style=\"text-align: left\">Data Type</th><td style=\"text-align: left\">float64</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(1000, 1001, 1002)</td></tr><tr><th style=\"text-align: left\">nnz</th><td style=\"text-align: left\">111648</td></tr><tr><th style=\"text-align: left\">Density</th><td style=\"text-align: left\">0.00011131383586473407</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Size</th><td style=\"text-align: left\">3.4M</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">0.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<COO: shape=(1000, 1001, 1002), dtype=float64, nnz=111648, fill_value=0.0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded = kruskal_to_tensor((weights, factors))\n",
    "expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the absolute error, both including and not including missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7267674362759795"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorly.contrib.sparse import norm\n",
    "norm((tensor - expanded)*mask) # Absolute error of the non-missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.606543216758244"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(tensor - expanded) # Absolute error including missing values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
