{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parafac\n",
    "\n",
    "Now the CANDECOMP/PARAFAC decomposition. The above tensor is too high a rank to reasonably decompose in this example, so we instead generate an example sparse tensor from a random sparse factorization and re-factor it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sparse\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1000, 1001, 1002, 100)\n",
    "rank = 5\n",
    "\n",
    "starting_factors = [sparse.random((i, rank)) for i in shape]\n",
    "starting_factors\n",
    "starting_weights = sparse.ones(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert it to a tensor. It is very important to use `kruskal_to_tensor` from the sparse backend, as a fully dense version of the tensor would use several TB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th style=\"text-align: left\">Format</th><td style=\"text-align: left\">coo</td></tr><tr><th style=\"text-align: left\">Data Type</th><td style=\"text-align: left\">float64</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(1000, 1001, 1002, 100)</td></tr><tr><th style=\"text-align: left\">nnz</th><td style=\"text-align: left\">5258</td></tr><tr><th style=\"text-align: left\">Density</th><td style=\"text-align: left\">5.242262727292668e-08</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Size</th><td style=\"text-align: left\">205.4K</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">0.0</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<COO: shape=(1000, 1001, 1002, 100), dtype=float64, nnz=5258, fill_value=0.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorly.contrib.sparse.kruskal_tensor import kruskal_to_tensor\n",
    "tensor = kruskal_to_tensor((starting_weights, starting_factors))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can compare the actual spase used by the tensor vs. what it would require if it were dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00021032"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.nbytes / 1e9                # Actual memory usage in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802.4016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(tensor.shape) * 8 / 1e9    # Memory usage if array was dense, in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import parafac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can decompose the tensor. Note again how much memory is actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot convert a sparse array to dense automatically. To manually densify, use the todense method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a2ccc3ec0a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"start_time = time.time()\\nfactors = parafac(tensor, rank=rank, init='random', verbose=True)\\nend_time = time.time()\\ntotal_time = end_time - start_time\\nprint('Took %d mins %d secs' % (divmod(total_time, 60)))\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-127>\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1013\u001b[0m                                \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                                \u001b[0mmax_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                                include_children=include_children)\n\u001b[0m\u001b[1;32m   1016\u001b[0m             \u001b[0mmem_usage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36m_func_exec\u001b[0;34m(stmt, ns)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;31m# helper for magic_memit, just a function proxy for the exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;31m# statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/tensorly/tensorly/decomposition/candecomp_parafac.py\u001b[0m in \u001b[0;36mparafac\u001b[0;34m(tensor, rank, n_iter_max, init, svd, normalize_factors, orthogonalise, tol, random_state, verbose, return_errors, non_negative, sparsity, l2_reg, mask, cvg_criterion, fixed_modes, linesearch)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mpseudo_inverse\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mId\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mmttkrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfolding_dot_khatri_rao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             factor = tl.transpose(tl.solve(tl.conj(tl.transpose(pseudo_inverse)),\n\u001b[1;32m    312\u001b[0m                                     tl.transpose(mttkrp)))\n",
      "\u001b[0;32m~/git_repos/tensorly/tensorly/kruskal_tensor.py\u001b[0m in \u001b[0;36munfolding_dot_khatri_rao\u001b[0;34m(tensor, kruskal_tensor, mode)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkruskal_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_mode_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mmttkrp_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/tensorly/tensorly/tenalg/__init__.py\u001b[0m in \u001b[0;36mdynamically_dispatched_fun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mcurrent_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_BACKENDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LOCAL_STATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtenalg_backend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             warnings.warn(f'tenalg: defaulting to core tenalg backend, {name}'\n",
      "\u001b[0;32m~/git_repos/tensorly/tensorly/tenalg/core_tenalg/n_mode_product.py\u001b[0m in \u001b[0;36mmulti_mode_dot\u001b[0;34m(tensor, matrix_or_vec_list, modes, skip, transpose)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_or_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_or_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_or_vec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/tensorly/tensorly/tenalg/core_tenalg/n_mode_product.py\u001b[0m in \u001b[0;36mmode_dot\u001b[0;34m(tensor, matrix_or_vector, mode)\u001b[0m\n\u001b[1;32m     62\u001b[0m                              'Provided array of dimension {} not in [1, 2].'.format(T.ndim(matrix_or_vector)))\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_or_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# We contracted with a vector, leading to a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/tensorly/tensorly/backend/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# We don't use `functools.wraps` here because some of the dispatched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/tensorly/tensorly/backend/numpy_backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sparse/_sparse_array.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mAUTO_DENSIFY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             raise RuntimeError(\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0;34m\"Cannot convert a sparse array to dense automatically. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;34m\"To manually densify, use the todense method.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot convert a sparse array to dense automatically. To manually densify, use the todense method."
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "start_time = time.time()\n",
    "factors = parafac(tensor, rank=rank, init='random', verbose=True)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print('Took %d mins %d secs' % (divmod(total_time, 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(factors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000, 5), (1001, 5), (1002, 5), (100, 5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in factors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though we started with a sparse tensor, the factors are dense. This is because we used the dense version of `parafac`. Since the factors are in general dense, even for a sparse tensor, this is generally preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the dense version of `parafac`. It should give the same answer, though it may be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.contrib.sparse.decomposition import parafac as parafac_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error=0.47643618036275764\n",
      "iteration 1, reconstruction error: 0.24060082750067524, decrease = 0.2358353528620824, unnormalized = 2.788771147576186\n",
      "iteration 2, reconstruction error: 0.21178785204968922, decrease = 0.028812975450986017, unnormalized = 2.4548039062818674\n",
      "iteration 3, reconstruction error: 0.21000039557804184, decrease = 0.0017874564716473806, unnormalized = 2.4340857438072825\n",
      "iteration 4, reconstruction error: 0.2089596003743366, decrease = 0.0010407952037052304, unnormalized = 2.422022029543371\n",
      "iteration 5, reconstruction error: 0.20823371325758314, decrease = 0.0007258871167534764, unnormalized = 2.4136083716660153\n",
      "iteration 6, reconstruction error: 0.20770079593574342, decrease = 0.0005329173218397154, unnormalized = 2.4074314001791404\n",
      "iteration 7, reconstruction error: 0.20729671601288788, decrease = 0.0004040799228555436, unnormalized = 2.4027477652894342\n",
      "iteration 8, reconstruction error: 0.20698307867445712, decrease = 0.00031363733843076114, unnormalized = 2.3991124379744613\n",
      "iteration 9, reconstruction error: 0.2067353504624973, decrease = 0.0002477282119598134, unnormalized = 2.3962410542924903\n",
      "iteration 10, reconstruction error: 0.2065371375147687, decrease = 0.00019821294772859321, unnormalized = 2.3939435952378245\n",
      "iteration 11, reconstruction error: 0.20637708684579206, decrease = 0.00016005066897664677, unnormalized = 2.3920884699149862\n",
      "iteration 12, reconstruction error: 0.20624706829968684, decrease = 0.00013001854610522612, unnormalized = 2.3905814428037555\n",
      "iteration 13, reconstruction error: 0.20614105234468844, decrease = 0.00010601595499840077, unnormalized = 2.3893526264295413\n",
      "iteration 14, reconstruction error: 0.20605440270649752, decrease = 8.664963819091098e-05, unnormalized = 2.3883482823736837\n",
      "iteration 15, reconstruction error: 0.20598343412551048, decrease = 7.096858098704084e-05, unnormalized = 2.3875256952982484\n",
      "iteration 16, reconstruction error: 0.2059251440619144, decrease = 5.8290063596089237e-05, unnormalized = 2.3868500631766314\n",
      "iteration 17, reconstruction error: 0.20587705519359062, decrease = 4.808886832377568e-05, unnormalized = 2.386292671708389\n",
      "iteration 18, reconstruction error: 0.2058371225500624, decrease = 3.993264352822323e-05, unnormalized = 2.3858298179215804\n",
      "iteration 19, reconstruction error: 0.20580367303140054, decrease = 3.3449518661854416e-05, unnormalized = 2.385442109144709\n",
      "iteration 20, reconstruction error: 0.20577535753987758, decrease = 2.83154915229622e-05, unnormalized = 2.3851139081713004\n",
      "iteration 21, reconstruction error: 0.20575110613502312, decrease = 2.4251404854458425e-05, unnormalized = 2.384832813468307\n",
      "iteration 22, reconstruction error: 0.20573008355232128, decrease = 2.1022582701840653e-05, unnormalized = 2.3845891435994395\n",
      "iteration 23, reconstruction error: 0.2057116460549052, decrease = 1.8437497416090576e-05, unnormalized = 2.3843754370990853\n",
      "iteration 24, reconstruction error: 0.2056953016920949, decrease = 1.6344362810283286e-05, unnormalized = 2.384185991834477\n",
      "iteration 25, reconstruction error: 0.20568067568887063, decrease = 1.4626003224271766e-05, unnormalized = 2.384016463839832\n",
      "iteration 26, reconstruction error: 0.20566748180381966, decrease = 1.3193885050971632e-05, unnormalized = 2.3838635353302466\n",
      "iteration 27, reconstruction error: 0.20565549963665014, decrease = 1.198216716952194e-05, unnormalized = 2.383724651676209\n",
      "iteration 28, reconstruction error: 0.20564455728240705, decrease = 1.0942354243093089e-05, unnormalized = 2.383597820350994\n",
      "iteration 29, reconstruction error: 0.20563451844468308, decrease = 1.0038837723963523e-05, unnormalized = 2.383481461561664\n",
      "iteration 30, reconstruction error: 0.2056252730656293, decrease = 9.245379053779423e-06, unnormalized = 2.383374299642807\n",
      "iteration 31, reconstruction error: 0.20561673061312344, decrease = 8.542452505866382e-06, unnormalized = 2.3832752852490273\n",
      "iteration 32, reconstruction error: 0.20560881531071373, decrease = 7.915302409705616e-06, unnormalized = 2.3831835400658794\n",
      "iteration 33, reconstruction error: 0.20560146275259056, decrease = 7.352558123174813e-06, unnormalized = 2.38309831757447\n",
      "iteration 34, reconstruction error: 0.20559461748815472, decrease = 6.84526443583966e-06, unnormalized = 2.383018975054519\n",
      "iteration 35, reconstruction error: 0.2055882312771692, decrease = 6.386210985531715e-06, unnormalized = 2.3829449533600644\n",
      "iteration 36, reconstruction error: 0.20558226180590972, decrease = 5.969471259470449e-06, unnormalized = 2.3828757620385397\n",
      "iteration 37, reconstruction error: 0.2055766717205743, decrease = 5.5900853354051705e-06, unnormalized = 2.3828109681271563\n",
      "iteration 38, reconstruction error: 0.20557142788062874, decrease = 5.243839945573425e-06, unnormalized = 2.3827501874985333\n",
      "iteration 39, reconstruction error: 0.20556650076695224, decrease = 4.927113676500561e-06, unnormalized = 2.382693078000596\n",
      "iteration 40, reconstruction error: 0.2055618640012849, decrease = 4.636765667337395e-06, unnormalized = 2.382639333886557\n",
      "iteration 41, reconstruction error: 0.20555749394791706, decrease = 4.370053367841464e-06, unnormalized = 2.382588681198151\n",
      "iteration 42, reconstruction error: 0.2055533693779997, decrease = 4.124569917357546e-06, unnormalized = 2.3825408738747047\n",
      "iteration 43, reconstruction error: 0.20554947118295974, decrease = 3.898195039958585e-06, unnormalized = 2.3824956904313725\n",
      "iteration 44, reconstruction error: 0.20554578212758498, decrease = 3.6890553747614963e-06, unnormalized = 2.3824529310971765\n",
      "iteration 45, reconstruction error: 0.20554228663575694, decrease = 3.4954918280427627e-06, unnormalized = 2.3824124153314665\n",
      "iteration 46, reconstruction error: 0.20553897060374413, decrease = 3.3160320128122756e-06, unnormalized = 2.3823739796598282\n",
      "iteration 47, reconstruction error: 0.205535821236869, decrease = 3.149366875121462e-06, unnormalized = 2.382337475780911\n",
      "iteration 48, reconstruction error: 0.20553282690635494, decrease = 2.994330514066723e-06, unnormalized = 2.382302768907162\n",
      "iteration 49, reconstruction error: 0.2055299770235688, decrease = 2.8498827861489495e-06, unnormalized = 2.382269736307189\n",
      "iteration 50, reconstruction error: 0.20552726192947884, decrease = 2.7150940899445253e-06, unnormalized = 2.382238266024482\n",
      "iteration 51, reconstruction error: 0.20552467279717004, decrease = 2.5891323088000906e-06, unnormalized = 2.382208255747481\n",
      "iteration 52, reconstruction error: 0.20552220154600678, decrease = 2.471251163260746e-06, unnormalized = 2.3821796118146477\n",
      "iteration 53, reconstruction error: 0.20551984076572574, decrease = 2.3607802810410305e-06, unnormalized = 2.3821522483346382\n",
      "iteration 54, reconstruction error: 0.20551758364938105, decrease = 2.2571163446893916e-06, unnormalized = 2.382126086409078\n",
      "iteration 55, reconstruction error: 0.2055154239339055, decrease = 2.1597154755415016e-06, unnormalized = 2.3821010534436153\n",
      "iteration 56, reconstruction error: 0.20551335584743957, decrease = 2.068086465939478e-06, unnormalized = 2.3820770825374185\n",
      "iteration 57, reconstruction error: 0.20551137406250927, decrease = 1.981784930299435e-06, unnormalized = 2.3820541119404686\n",
      "iteration 58, reconstruction error: 0.2055094736543652, decrease = 1.900408144073218e-06, unnormalized = 2.3820320845706715\n",
      "iteration 59, reconstruction error: 0.2055076500638829, decrease = 1.823590482286086e-06, unnormalized = 2.3820109475838422\n",
      "iteration 60, reconstruction error: 0.205505899064382, decrease = 1.7509995009212798e-06, unnormalized = 2.3819906519891085\n",
      "iteration 61, reconstruction error: 0.20550421673200647, decrease = 1.6823323755188468e-06, unnormalized = 2.3819711523055944\n",
      "iteration 62, reconstruction error: 0.20550259941915047, decrease = 1.617312856000419e-06, unnormalized = 2.3819524062544013\n",
      "iteration 63, reconstruction error: 0.2055010437306281, decrease = 1.555688522364429e-06, unnormalized = 2.381934374482396\n",
      "iteration 64, reconstruction error: 0.20549954650224614, decrease = 1.4972283819691956e-06, unnormalized = 2.3819170203138493\n",
      "iteration 65, reconstruction error: 0.20549810478150216, decrease = 1.441720743983188e-06, unnormalized = 2.381900309526711\n",
      "iteration 66, reconstruction error: 0.2054967158101858, decrease = 1.3889713163517392e-06, unnormalized = 2.3818842101509436\n",
      "iteration 67, reconstruction error: 0.2054953770086493, decrease = 1.3388015364934613e-06, unnormalized = 2.3818686922862042\n",
      "iteration 68, reconstruction error: 0.2054940859615882, decrease = 1.291047061119377e-06, unnormalized = 2.3818537279370307\n",
      "iteration 69, reconstruction error: 0.20549284040514781, decrease = 1.2455564403768271e-06, unnormalized = 2.381839290863394\n",
      "iteration 70, reconstruction error: 0.20549163821520433, decrease = 1.2021899434833117e-06, unnormalized = 2.3818253564448675\n",
      "iteration 71, reconstruction error: 0.2054904773967355, decrease = 1.1608184688205458e-06, unnormalized = 2.3818119015574215\n",
      "iteration 72, reconstruction error: 0.20548935607415142, decrease = 1.1213225840911445e-06, unnormalized = 2.381798904461351\n",
      "iteration 73, reconstruction error: 0.20548827248244758, decrease = 1.0835917038376497e-06, unnormalized = 2.3817863446987317\n",
      "iteration 74, reconstruction error: 0.20548722495915642, decrease = 1.0475232911644206e-06, unnormalized = 2.38177420300013\n",
      "iteration 75, reconstruction error: 0.20548621193697952, decrease = 1.0130221768933634e-06, unnormalized = 2.3817624611992056\n",
      "iteration 76, reconstruction error: 0.20548523193705748, decrease = 9.799999220461153e-07, unnormalized = 2.381751102154702\n",
      "iteration 77, reconstruction error: 0.20548428356277487, decrease = 9.483742826055241e-07, unnormalized = 2.3817401096786415\n",
      "iteration 78, reconstruction error: 0.20548336549411822, decrease = 9.180686566523377e-07, unnormalized = 2.381729468470929\n",
      "iteration 79, reconstruction error: 0.2054824764824498, decrease = 8.890116684201477e-07, unnormalized = 2.3817191640587767\n",
      "iteration 80, reconstruction error: 0.20548161534574416, decrease = 8.611367056376995e-07, unnormalized = 2.38170918274149\n",
      "iteration 81, reconstruction error: 0.2054807809641622, decrease = 8.34381581965582e-07, unnormalized = 2.381699511539167\n",
      "iteration 82, reconstruction error: 0.20547997227599538, decrease = 8.086881668201151e-07, unnormalized = 2.38169013814569\n",
      "iteration 83, reconstruction error: 0.20547918827393094, decrease = 7.840020644356294e-07, unnormalized = 2.3816810508854354\n",
      "iteration 84, reconstruction error: 0.20547842800157434, decrease = 7.602723565980352e-07, unnormalized = 2.3816722386729654\n",
      "iteration 85, reconstruction error: 0.20547769055026177, decrease = 7.374513125713023e-07, unnormalized = 2.3816636909760827\n",
      "iteration 86, reconstruction error: 0.20547697505610307, decrease = 7.154941586984265e-07, unnormalized = 2.3816553977815547\n",
      "iteration 87, reconstruction error: 0.20547628069721693, decrease = 6.943588861385575e-07, unnormalized = 2.3816473495630683\n",
      "iteration 88, reconstruction error: 0.2054756066912256, decrease = 6.74005991324611e-07, unnormalized = 2.3816395372521906\n",
      "iteration 89, reconstruction error: 0.20547495229284915, decrease = 6.543983764595307e-07, unnormalized = 2.381631952210484\n",
      "iteration 90, reconstruction error: 0.20547431679175487, decrease = 6.355010942760142e-07, unnormalized = 2.3816245862045795\n",
      "iteration 91, reconstruction error: 0.20547369951049543, decrease = 6.172812594407162e-07, unnormalized = 2.381617431382278\n",
      "iteration 92, reconstruction error: 0.2054730998026101, decrease = 5.997078853237081e-07, unnormalized = 2.381610480250543\n",
      "iteration 93, reconstruction error: 0.2054725170508799, decrease = 5.827517302048335e-07, unnormalized = 2.381603725655273\n",
      "iteration 94, reconstruction error: 0.20547195066565832, decrease = 5.663852215842535e-07, unnormalized = 2.381597160761957\n",
      "iteration 95, reconstruction error: 0.20547140008334652, decrease = 5.505823117979425e-07, unnormalized = 2.3815907790379973\n",
      "iteration 96, reconstruction error: 0.20547086476496584, decrease = 5.353183806788842e-07, unnormalized = 2.381584574236167\n",
      "iteration 97, reconstruction error: 0.20547034419479665, decrease = 5.205701691934905e-07, unnormalized = 2.3815785403788308\n",
      "iteration 98, reconstruction error: 0.20546983787914727, decrease = 5.063156493789744e-07, unnormalized = 2.381572671743676\n",
      "iteration 99, reconstruction error: 0.2054693453451751, decrease = 4.925339721628674e-07, unnormalized = 2.381566962850049\n",
      "Took 2 mins 9 secs\n",
      "peak memory: 136.34 MiB, increment: 24.07 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "start_time = time.time()\n",
    "sparse_kruskal = parafac_sparse(tensor, rank=rank, init='random', verbose=True)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print('Took %d mins %d secs' % (divmod(total_time, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights, factors) : rank-5 KruskalTensor of shape (1000, 1001, 1002, 100) "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_kruskal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `factors_sparse` are sparse, we can reconstruct them into a tensor without using too much memory. In general, this will not be the case, but it is for our toy example. Let's do this to look at the absolute error for the decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3815669628500578"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(tensor - kruskal_to_tensor(sparse_kruskal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not actually necessary to compute this, as the same as the norm of the tensor times the reconstruction error that was printed by the algorithm (you can pass `return_errors=True` to `parafac()` to have the reconstruction errors be returned along with the factors). That is, $$\\mathrm{reconstruction\\ error} = \\frac{\\|\\mathrm{tensor} - \\mathrm{kruskal\\_to\\_tensor}(\\mathrm{factors})\\|_2}{\\|\\mathrm{tensor}\\|_2}$$ (they won't be exactly the same due to numerical differences in how they are calculated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20546934534517586"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.norm(tensor - kruskal_to_tensor(sparse_kruskal))/tl.norm(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the nonzero entries to see how close it is to the original tensor. The factors satisfy $$\\sum_{r=0}^{R-1} {f_0}_r\\circ {f_1}_r \\circ {f_2}_r \\circ {f_3}_r,$$ where $R$ is the rank (here 5), ${f_i}_r$ is the $r$-th column of the $i$-th factor of the decomposition, and $\\circ$ is the vector outer product. Component-wise, this translates to a product of corresponding elements per component for each factor, summed over the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103, 103, 103, ..., 930, 930, 930],\n",
       "       [204, 204, 204, ..., 905, 905, 905],\n",
       "       [ 77,  77,  77, ..., 963, 963, 963],\n",
       "       [ 14,  29,  56, ...,  14,  29,  56]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016216060526249777"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_val = tensor[tuple(tensor.coords.T[0])]\n",
    "orig_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'factors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0dc76037af7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdense_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0dc76037af7a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdense_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'factors' is not defined"
     ]
    }
   ],
   "source": [
    "dense_val = np.sum(np.prod(np.stack([factors[i][idx] for i, idx in enumerate(tuple(tensor.coords.T[0]))], 0), 0))\n",
    "dense_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same for the sparse factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_sparse, factors_sparse = sparse_kruskal\n",
    "sparse_val = np.sum(np.prod(sparse.stack([factors_sparse[i][idx] for i, idx in enumerate(tuple(tensor.coords.T[0]))], 0), 0))\n",
    "sparse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(orig_val - dense_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(orig_val - sparse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here is mostly due to random chance. The total reconstruction errors for the two runs of algorithm were roughly the same. In general, the error of the factorization will vary due to the randomness of the initial factors chosen by the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
